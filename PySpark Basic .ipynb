{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating your dataframe"
   ]
  },
 
   "source": [
    "#Method--1\n",
    "from pyspark.sql import Row\n",
    "l = Row('name','id')\n",
    "l1 = l('Person1',3.0)\n",
    "l2 = l('Person2', 4.5)\n",
    "team = [l1,l2]\n",
    "df1 = spark.createDataFrame(team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method--2\n",
    "d1 = Row(id='123456', course='NLP',name='Person1')\n",
    "d2 = Row(id='789012', course='ML',name='Person2')\n",
    "d3 = Row(id='345678', course='ML',name='Person3')\n",
    "d4 = Row(id='901234', course='AI',name='Person4')\n",
    "d5 = Row(id='901234', course='Hadoop',name='Person5')\n",
    "d6 = Row(id='901234', course='AI',name='Person6')\n",
    "d = [d1,d2,d3,d4,d5,d6]\n",
    "df2 = spark.createDataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the created dataframe\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = df1.join(df2,on='name',how='outer')\n",
    "merged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joins exclusively offered by spark\n",
    "semi_merged = df1.join(df2,on='name',how='left_semi') \n",
    "#Will output rows from df1 for which names from df1 match with names from df2\n",
    "semi_merged.show()\n",
    "\n",
    "anti_merged = df1.join(df2,on='name',how='left_anti') \n",
    "#Will output rows from df1 wherein names from df1 do not match in df2\n",
    "anti_merged.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering/where condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "where_person1 = merged.where(col('name') == 'Person1') #using where condition\n",
    "where_person1.show()\n",
    "\n",
    "filter_person2 = merged.filter(col('name') == 'Person2')#using filter condition\n",
    "filter_person2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selective_cols = merged.select(['name','course'])#using select to filter out selective columns\n",
    "selective_cols.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TypeCasting dataframe column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType    \n",
    "#for single column\n",
    "df1 = df1.withColumn('id', df1['id'].cast('String')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For typecasting multiple columns of the dataframe\n",
    "cols = df1.columns\n",
    "df1 = df1.select([col(c).cast(StringType() for c in cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Groupby\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg, count, first\n",
    "\n",
    "# create toy example dataframe with column 'A', 'B' and 'C'\n",
    "ls = [['a', 'b',3], ['a', 'b', 4], ['a', 'c', 3], ['b', 'b', 5]]\n",
    "df3 = spark.createDataFrame(ls, schema=['A', 'B', 'Value'])\n",
    "\n",
    "# group by column 'A' and 'B' then performing some function here\n",
    "group_df = df3.groupby(['A']).agg(sum('Value').alias('sum_Values'),avg('Values').alias('avg_Values')\n",
    "df_grouped = group_df.agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f \n",
    "merged.groupby(['course']).agg(f.collect_set('name')).show()\n",
    "#This will create a list of unique names falling under the same course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "merged = merged.withColumn('newColumn1',lit(\"This is a new df column\"))\n",
    "merged.show()\n",
    "\n",
    "#This will create a new column with name 'newColumn1' and text as 'This is a new df column'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using when..otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "merged = merged.withColumn('Suggested_Course',when(col('course')=='ML','Deep Learning').otherwise('Software Engg'))\n",
    "merged.show()\n",
    "\n",
    "\"\"\"\n",
    "A new column 'Suggested_course' will be created which will have the value 'Deep Learning' if the column course\n",
    "has value as 'ML' else it will have the value as 'Software Engg'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
